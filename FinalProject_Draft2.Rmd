---
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **Predicting Lebron James's Shot Outcomes**
#### Joey Keating, Jack Kuczmanski, Aroon Prabhu, Alex Sooch, Nikhil Venkata  
#####  TO 414 - Final Project - April 2020

----- 

```{r warning=FALSE, message=FALSE}
library(dplyr); library(class); library(caret); library(kernlab); library(neuralnet); library(C50)

shot_logs <- read.csv("shot_logs.csv")
```

----- 

#####  **About the Data**

The data used for this project, titled "NBA Shot Logs," was sourced from kaggle.com. The data contains information on shots taken during the 2014-2015 NBA season. For each record in the data set, the following information is available: who took the shot, the distance the player was from the basket at the time of the shot, who the nearest defender was, how far away the nearest defender was, time on the clock, and more. 

The goal in this project is two fold:

1. Idenfity the variables that have an impact on a player's ability to make a shot

2. Use Machine Learning Algorithms to predict shot results for a specific player, Lebron James   

----- 

#####  **Data Processing**
Before exploring the data and building models, certain processing measures should be taken. 

```{r}
str(shot_logs)
```

In observing the structure of the data, there are columns that contain redundant information, or information that won't be relevant in prediction. Those columns can be removed.

```{r}
shot_logs$GAME_ID<-NULL
shot_logs$W<-NULL
shot_logs$GAME_CLOCK<-NULL
shot_logs$SHOT_RESULT<-NULL
shot_logs$CLOSEST_DEFENDER_PLAYER_ID<-NULL
shot_logs$player_id<-NULL
shot_logs$PTS<-NULL
```


Secondly, there are columns that should be re-coded as factors, including the response variable, ` FGM`, which indicates whether or not the shot was made. 

```{r}
shot_logs$FGM<-as.factor(shot_logs$FGM)
shot_logs$PERIOD<-as.factor(shot_logs$PERIOD)
shot_logs$PTS_TYPE<-as.factor(shot_logs$PTS_TYPE)
```

----- 

#####  **Testing Significance with Logistic Regression**

With initial processing complete, a logistic regression can be used to identify which variables are significant in predicting whether a player will make his shot. 

We will look at all the variables we have: 

* ` DRIBBLES`: number of dribbles the player took before shooting 
* ` TOUCH_TIME`: time the player took to shoot after receiveing the ball (seconds)
* ` SHOT_DIST` : distance the player was from the basket at the time of the shot (feet)
* ` CLOSE_DEF_DIST`: the distance between the shooter and the nearest defender (feet)
* ` SHOT_CLOCK`: time remaining on shot clock (seconds)
* ` SHOT_NUMBER`: a counter representing the amount of shots the player has taken in a specific game
* ` FINAL_MARGIN`: final margin of the game, with negative numbers indicating the shooter's team lost
* ` LOCATION`: whether the game was played at the shooter's home arena or on the road
* ` PERIOD`: the period of the game, with numbers greater than 4 indicating overtime 

```{r}
log_model<-glm(FGM ~ DRIBBLES + TOUCH_TIME + SHOT_DIST + CLOSE_DEF_DIST + SHOT_CLOCK + 
                     SHOT_NUMBER + FINAL_MARGIN + LOCATION + PERIOD, 
                     data=shot_logs, family="binomial")

summary(log_model)
```

The output of the logistic model shows that most of the variables are highly significant. The only variable that does not have any significance related to it is ` LOCATION`. However, this could change when looking at individual players. Overall, the logisitc model shows that there are plenty of variables that have predictive ability. 

----- 

#####  **Subsetting Lebron**
Basketball is a very context specific game - from player to player, things like shot selection and the "make-ability" of a particular shot will vary. The purpose of being able to predict the success of a shot attempt would be to identify "good" shots vs. "bad" shots. Because evey shot taken in an NBA game is context dependent, it doesn't necessarily make sense to make predictions on a league-wide basis. 

Lebron James's data will be used for model building and prediction. 

```{r}
lebron<-shot_logs %>% filter(player_name=="lebron james")
```

----- 

#####  **Exploring James's Data**

In the 2014-2015 NBA Season, James appeared in 69 games. Unfortunately, only **`r n_distinct(lebron$MATCHUP)`** of his games are present in the data set.

In these games, Lebron James...

* took **`r nrow(lebron)`** total shots 
* took **`r nrow(lebron[lebron$PTS_TYPE=="2", ] )`** 2 point shots  
* took **`r nrow(lebron[lebron$PTS_TYPE=="3", ] )`** 3 point shots  
* had a field goal percentage of **`r paste(round(nrow(lebron[lebron$FGM==1, ]) / nrow(lebron) * 100), "%")`**

In addition to these basic statistcs, we can visualize James's shot selection. 

```{r fig.align="center", fig.width=12, warning=FALSE, message=FALSE}
ggplot(data=lebron)+ 
  geom_histogram(aes(x=SHOT_DIST), color="navyblue", fill="darkred")+ 
  labs(x="Distance From Basket (ft.)", 
       y="Total Shots", 
       title="Lebron James Shot Distribution by Distance from Basket")+
  theme_bw()

```

This chart conveys that James refrains from shooting mid-range shots, with the overwhelming majority of his attempts coming from within 8 feet or beyond 22. There also may be shot selection patterns based on whether James is playing in his home arena.  

```{r fig.align="center", fig.width=12, warning=FALSE, message=FALSE}
ggplot(data=lebron)+ 
  geom_histogram(aes(x=SHOT_DIST, fill=LOCATION), color="darkred")+ 
  labs(x="Distance From Basket (ft.)", 
       y="Total Shots", 
       title="Lebron James Shot Distribution by Distance from Basket")+
  facet_wrap(~LOCATION)+
  scale_fill_manual(values = c("H" = "yellow" , "A" = "navyblue"))+ 
  theme_bw()
```

The distributions are largely similar. It appears that James does not alter his shot selection based on whether he is playing at home or on the road. 


***Lets put some more ggplots here I think ***

With a better understanding of the data, we can continue processing the data to prepare it for various machine learning algorithms. 

----- 

#####  **Further Data Processing**
Data processing will be continued by randomizing, removing factors, normalizing, and creating training and testing data sets.
```{r}
set.seed(13)

# randomizing data & dropping unnecesary columns
lebron1<-lebron[sample(nrow(lebron)), ] %>% 
  select(-MATCHUP, -CLOSEST_DEFENDER, -PTS_TYPE, - player_name, -PERIOD)

# using model.matric to create dummy variables for factors - which will be necesarry for certain ML algorithms
lebron1<-as.data.frame(model.matrix(~. -1, data=lebron1))

# create normalization function 
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

# normalize the data
lebron_norm <- as.data.frame(lapply(lebron1, normalize))

# re-code variable of interest to factor 
lebron_norm$FGM1<-as.factor(lebron_norm$FGM1)

# create split point - train on 80% of data, test on 20% of data
split<-round(.80*(nrow(lebron_norm)))

# train data
lebron_train<-lebron_norm[1:split, ]

# test data 
lebron_test<-lebron_norm[(split+1): nrow(lebron_norm), ]
```

----- 

#####  **Model Building**
With the data processing complete, we can being building models for prediction. The following models will be trained and tested: 

* **kNN** - with k = 3 
* **SVM** - with vanilla kernel 
* **SVM** - with a radial kernel
* **Neural Network** 
* **Neural Netwwork** - with 5 hidden layers 
* **Decision Tree** 
* **Decision Tree** - boosted with 10 trials 

Recalling from earlier, Lebron James had a 49% field goal percentage in the games included in this data. This is important information for all of the models to be considered against. If we simply guessed that Lebron missed every one of his shots, we would be 51% accurate. Any "good" model should have a higher accuracy %. Given the inherent randomness associated with making a shot, any improvement over 51% is decent. 

To quickly assess each model, the accuracy of each model will be printed. 

----- 

#####  **kNN**
```{r}
# create train and test for kNN by removing removing response variable 
knn_train<-lebron_train %>% select(-FGM1)
knn_test<-lebron_test %>% select(-FGM1)

# create train and test labels 
knn_train_labels<-lebron_train[ , "FGM1"]
knn_test_labels<-lebron_test[ , "FGM1"]

# make kNN predictions 
knn_predictions <- knn(train = knn_train, test = knn_test, cl = knn_train_labels, k=3)

# calculate kNN accuracy
round(confusionMatrix(knn_test_labels, knn_predictions)$overall["Accuracy"]*100, 1)
```

----- 

#####  **SVM - Vanilla Kernel**
```{r}
# build SVM - Vanilla Kernel classifier 
svm_classifier <- ksvm(FGM1 ~ ., data = lebron_train, kernel = "vanilladot", kpar=list())

# make SVM - Vanilla Kernel predcitions
svm_predictions <- predict(svm_classifier, lebron_test)

# calculate SVM - Vanilla Kernel accuracy 
round(confusionMatrix(lebron_test$FGM1, svm_predictions)$overall["Accuracy"]*100, 1)
```


#####  **SVM - Radial Kernel**
```{r}
# build SVM - Radial Kernel classifier 
svm_classifier2 <- ksvm(FGM1 ~ ., data = lebron_train, kernel = "rbfdot", kpar=list())

# make SVM - Radial Kernel predictions 
svm_predictions2 <- predict(svm_classifier2, lebron_test)

# calculate SVM - Radial Kernel accuracy 
round(confusionMatrix(lebron_test$FGM1, svm_predictions2)$overall["Accuracy"]*100, 1)
```

----- 

#####  **Neural Network**
```{r}
# build Neural Network classifer 
ann_classifier <- neuralnet(FGM1 ~ ., data = lebron_train)

# make Neural Network predicitons
ann_test<-lebron_test %>% select(-FGM1)

ann_results <- compute(ann_classifier, ann_test)

ann_predictions<-as.data.frame(ann_results$net.result) %>% 
  mutate(ann_pred=as.factor(ifelse(V2 > V1, 1, 0))) %>% select(ann_pred)

# calculate Neural Network accuracy 
round(confusionMatrix(lebron_test$FGM1, ann_predictions$ann_pred)$overall["Accuracy"]*100, 1)
```


#####  **Neural Network - 5 Hidden Layers**
```{r}
# build Neural Network - 5 Hidden Layers classifer 
ann_classifier2 <- neuralnet(FGM1 ~ ., data = lebron_train, hidden = 5)

# make Neural Network - 5 Hidden Layers predicitons
ann_test<-lebron_test %>% select(-FGM1)

ann_results2 <- compute(ann_classifier2, ann_test)

ann_predictions2<-as.data.frame(ann_results2$net.result) %>% 
  mutate(ann_pred=as.factor(ifelse(V2 > V1, 1, 0))) %>% select(ann_pred)

# calculate Neural Network - 5 Hidden Layers accuracy 
round(confusionMatrix(lebron_test$FGM1, ann_predictions2$ann_pred)$overall["Accuracy"]*100, 1)
```

----- 

#####  **Decision Tree**
```{r fig.width=15}
# build Decision Tree classifier 
dec_classifier <- C5.0(FGM1 ~ ., data = lebron_train)

# make Decision Tree predictions 
dec_predictions <- predict(dec_classifier, lebron_test)

# calculate Decision Tree accuracy 
round(confusionMatrix(lebron_test$FGM1, dec_predictions)$overall["Accuracy"]*100, 1)
```


#####  **Decision Tree - Boosted**
```{r}
# build Decision Tree - Boosted classifier 
dec10_classifier <- C5.0(FGM1 ~ ., data = lebron_train, trials=10)

# make Decision Tree - Boosted predictions 
dec10_predictions <- predict(dec10_classifier, lebron_test)

# calculate Decision Tree - Boosted accuracy 
round(confusionMatrix(lebron_test$FGM1, dec10_predictions)$overall["Accuracy"]*100, 1)
```

----- 

#####  **Model Improvement**

6 of the 7 models yielded an accuracy % greater than the baseline 51%, with the exception of the kNN. Some of the models did exceptionally well, with the **SVM - Vanilla Kernel**, **Neural Network**, and **Decision Tree**, having accuracy %'s of **69%**, **66%**, and **68%**, respectively. In an effort to improve the models, a couple of different techniques should be employed. 

----

#####  **Stacked Model**
A stacked model will be used in an attempt to improve predictions. Each of the the 7 models created previously will be tested on the *training* data set. After this re-testing, these predictions will be used as inputs in a stacked decision tree. 
```{r}
# creating a data from of all predictions made on the testing data
# wil be used later when testing the stacked tree 
testing_predictions<-data.frame(knn_predictions,
                                svm_predictions,
                                svm_predictions2,
                                ann_predictions,
                                ann_predictions2,
                                dec_predictions,
                                dec10_predictions)


# making predictions on training dataset

# ... kNN
knn_predictions_train <- knn(train = knn_train, test = knn_train, cl = knn_train_labels, k=3)

# ... SVM - Vanilla Kernel
svm_predictions_train <- predict(svm_classifier, lebron_train)

# ... SVM - Radial Kernel
svm_predictions_train2 <- predict(svm_classifier2, lebron_train)

# ... Neural Network 
ann_train<-lebron_train %>% select(-FGM1)

ann_results_train <- compute(ann_classifier, ann_train)

ann_predictions_train<-as.data.frame(ann_results_train$net.result) %>%
  mutate(ann_pred=as.factor(ifelse(V2 > V1, 1, 0))) %>% select(ann_pred)

# ... Neural Network - 5 Hidden Layers
ann_results_train2 <- compute(ann_classifier2, ann_train)

ann_predictions_train2<-as.data.frame(ann_results_train2$net.result) %>%
  mutate(ann_pred=as.factor(ifelse(V2 > V1, 1, 0))) %>% select(ann_pred)

# ... Decision Tree
dec_predictions_train <- predict(dec_classifier, lebron_train)

# ... Decision Tree - Boosted
dec10_predictions_train <- predict(dec10_classifier, lebron_train)

# creating a data from of all predictions made on the training data
training_predictions<-data.frame(knn_predictions_train,
                                 svm_predictions_train,
                                 svm_predictions_train2,
                                 ann_predictions_train,
                                 ann_predictions_train2,
                                 dec_predictions_train,
                                 dec10_predictions_train)

# making names match
names(training_predictions)<-names(testing_predictions)

#  building Stacked Decision Tree
lebron_stacked_tree<-C5.0(lebron_train$FGM1 ~ ., data = training_predictions)

# make Stacked Decision Tree predicitons 
stacked_tree_predictions<-predict(lebron_stacked_tree, testing_predictions)

# calculate Stacked Decision Tree accuracy 
round(confusionMatrix(lebron_test$FGM1, stacked_tree_predictions)$overall["Accuracy"]*100, 1)
```



```{r}
```

