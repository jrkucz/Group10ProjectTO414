---
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **Lebron vs Harden - Comparing Shot Selections Using Machine Learning**
#### Joey Keating, Jack Kuczmanski, Aroon Prabhu, Alex Sooch, Nikhil Venkata  
#####  TO 414 - Final Project - April 2020

----- 

```{r warning=FALSE, message=FALSE}
library(dplyr); library(class); library(caret); library(kernlab); library(neuralnet); 
library(C50); library(randomForest); library(ipred); library(gridExtra)

shot_logs <- read.csv("shot_logs.csv")
```

----- 

#####  **About the Data**

The data used for this project, titled "NBA Shot Logs," was sourced from kaggle.com. The data contains information on shots taken during the 2014-2015 NBA season. For each record in the data set, the following information is available: who took the shot, the distance the player was from the basket at the time of the shot, who the nearest defender was, how far away the nearest defender was, time on the clock, and more. 

The goal of this project is two fold:

1. Use Machine Learning Algorithms to predict shot results for Lebron James 

2. Compare James' shot selection to those of James Harden

----- 

#####  **Data Processing**
Before exploring the data and building models, certain processing measures should be taken. 

```{r}
str(shot_logs)
```

In observing the structure of the data, there are columns that contain redundant information, or information that won't be relevant in prediction. Those columns can be removed.

```{r}
shot_logs$GAME_ID<-NULL
shot_logs$W<-NULL
shot_logs$GAME_CLOCK<-NULL
shot_logs$SHOT_RESULT<-NULL
shot_logs$CLOSEST_DEFENDER_PLAYER_ID<-NULL
shot_logs$player_id<-NULL
shot_logs$PTS<-NULL
```


Secondly, there are columns that should be re-coded as factors, including the response variable, ` FGM`, which indicates whether or not the shot was made. 

```{r}
shot_logs$FGM<-as.factor(shot_logs$FGM)
shot_logs$PERIOD<-as.factor(shot_logs$PERIOD)
shot_logs$PTS_TYPE<-as.factor(shot_logs$PTS_TYPE)
```

----- 

#####  **Testing Significance with Logistic Regression**

With initial processing complete, a logistic regression can be used to identify which variables are significant in predicting whether a player will make his shot. 

We will look at all the variables we have: 

* ` DRIBBLES`: number of dribbles the player took before shooting 
* ` TOUCH_TIME`: time the player took to shoot after receiveing the ball (seconds)
* ` SHOT_DIST` : distance the player was from the basket at the time of the shot (feet)
* ` CLOSE_DEF_DIST`: the distance between the shooter and the nearest defender (feet)
* ` SHOT_CLOCK`: time remaining on shot clock (seconds)
* ` SHOT_NUMBER`: a counter representing the amount of shots the player has taken in a specific game
* ` FINAL_MARGIN`: final margin of the game, with negative numbers indicating the shooter's team lost
* ` LOCATION`: whether the game was played at the shooter's home arena or on the road
* ` PERIOD`: the period of the game, with numbers greater than 4 indicating overtime 

```{r}
log_model<-glm(FGM ~ DRIBBLES + TOUCH_TIME + SHOT_DIST + CLOSE_DEF_DIST + SHOT_CLOCK + 
                     SHOT_NUMBER + FINAL_MARGIN + LOCATION + PERIOD, 
                     data=shot_logs, family="binomial")

summary(log_model)
```

The output of the logistic model shows that most of the variables are highly significant. The only variable that does not have any significance related to it is ` LOCATION`. However, this could change when looking at individual players. Overall, the logisitc model shows that there are plenty of variables that have predictive ability. 

----- 

#####  **Subsetting Lebron James**
Basketball is a very context specific game - from player to player, things like shot selection and the "make-ability" of a particular shot will vary. The purpose of being able to predict the success of a shot attempt would be to identify "good" shots vs. "bad" shots. Because evey shot taken in an NBA game is context dependent, it doesn't necessarily make sense to make predictions on a league-wide basis, which is why we will only look at individual player. 

```{r}
lebron<-shot_logs %>% filter(player_name=="lebron james")
```

----- 

#####  **Exploring James's Data**

In the 2014-2015 NBA Season, James appeared in 69 games. Unfortunately, only **`r n_distinct(lebron$MATCHUP)`** of his games are present in the data set.

In these games, Lebron James...

* took **`r nrow(lebron)`** total shots 
* took **`r nrow(lebron[lebron$PTS_TYPE=="2", ] )`** 2 point shots  
* took **`r nrow(lebron[lebron$PTS_TYPE=="3", ] )`** 3 point shots  
* had a field goal percentage of **`r paste(round(nrow(lebron[lebron$FGM==1, ]) / nrow(lebron) * 100), "%")`**

In addition to these basic statistcs, we can visualize James's shot selection. 

```{r fig.align="center", fig.width=12, warning=FALSE, message=FALSE}
ggplot(data=lebron)+ 
  geom_histogram(aes(x=SHOT_DIST), color="navyblue", fill="darkred")+ 
  labs(x="Distance From Basket (ft.)", 
       y="Total Shots", 
       title="Lebron James Shot Distribution by Distance from Basket")+
  theme_bw()

```

This chart conveys that James refrains from shooting mid-range shots, with the overwhelming majority of his attempts coming from within 8 feet or beyond 22. There also may be shot selection patterns based on whether James is playing in his home arena.  

```{r fig.align="center", fig.width=12, warning=FALSE, message=FALSE}
ggplot(data=lebron)+ 
  geom_histogram(aes(x=SHOT_DIST, fill=LOCATION), color="darkred")+ 
  labs(x="Distance From Basket (ft.)", 
       y="Total Shots", 
       title="Lebron James Shot Distribution by Distance from Basket")+
  facet_wrap(~LOCATION)+
  scale_fill_manual(values = c("H" = "yellow" , "A" = "navyblue"))+ 
  theme_bw()
```

The distributions are largely similar. It appears that James does not alter his shot selection based on whether he is playing at home or on the road. 

```{r fig.align="center", fig.width=12, warning=FALSE, message=FALSE}
ggplot(data=lebron)+ 
  geom_histogram(aes(x=SHOT_CLOCK), color="navyblue", fill="darkred")+ 
  labs(x="Shot Clock (seconds)", 
       y="Total Shots", 
       title="Lebron James Shot Distribution by Time Left on Shot Clock")+
  theme_bw()
```

Furthermore, Lebron's shots appear to be farily normally distributed by time on left on shot clock. 

With a better understanding of the data, we can continue processing the data to prepare it for various machine learning algorithms. 

----- 

#####  **Further Data Processing**
Data processing will be continued by randomizing, removing factors, normalizing, and creating training and testing data sets.
```{r}
set.seed(13)

# randomizing data & dropping unnecesary columns
lebron1<-lebron[sample(nrow(lebron)), ] %>% 
  select(-MATCHUP, -CLOSEST_DEFENDER, -PTS_TYPE, - player_name, -PERIOD)

# using model.matric to create dummy variables for factors - which will be necesarry for certain ML algorithms
lebron1<-as.data.frame(model.matrix(~. -1, data=lebron1))

# create normalization function 
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

# normalize the data
lebron_norm <- as.data.frame(lapply(lebron1, normalize))

# re-code variable of interest to factor 
lebron_norm$FGM1<-as.factor(lebron_norm$FGM1)

# create split point - train on 80% of data, test on 20% of data
split<-round(.80*(nrow(lebron_norm)))

# train data
lebron_train<-lebron_norm[1:split, ]

# test data 
lebron_test<-lebron_norm[(split+1): nrow(lebron_norm), ]
```

----- 

#####  **Model Building**
With the data processing complete, we can being building models for prediction. The following models will be trained and tested: 

* **kNN** - with k = 3 
* **SVM** - with vanilla kernel 
* **SVM** - with a radial kernel
* **Neural Network** 
* **Neural Netwwork** - with 5 hidden layers 
* **Decision Tree** 
* **Decision Tree** - boosted with 10 trials 

Recalling from earlier, Lebron James had a 49% field goal percentage in the games included in this data. This is important information for all of the models to be considered against. If we simply guessed that Lebron missed every one of his shots, we would be 51% accurate. Any "good" model should have a higher accuracy %. Given the inherent randomness associated with making a shot, any improvement over 51% is decent. 

To quickly assess each model, the accuracy of each model will be printed. 

----- 

#####  **kNN**
```{r}
# create train and test for kNN by removing removing response variable 
knn_train<-lebron_train %>% select(-FGM1)
knn_test<-lebron_test %>% select(-FGM1)

# create train and test labels 
knn_train_labels<-lebron_train[ , "FGM1"]
knn_test_labels<-lebron_test[ , "FGM1"]

# make kNN predictions 
knn_predictions <- knn(train = knn_train, test = knn_test, cl = knn_train_labels, k=3)

# calculate kNN accuracy
round(confusionMatrix(knn_test_labels, knn_predictions)$overall["Accuracy"]*100, 1)
```

----- 

#####  **SVM - Vanilla Kernel**
```{r}
# build SVM - Vanilla Kernel classifier 
svm_classifier <- ksvm(FGM1 ~ ., data = lebron_train, kernel = "vanilladot", kpar=list())

# make SVM - Vanilla Kernel predcitions
svm_predictions <- predict(svm_classifier, lebron_test)

# calculate SVM - Vanilla Kernel accuracy 
round(confusionMatrix(lebron_test$FGM1, svm_predictions)$overall["Accuracy"]*100, 1)
```


#####  **SVM - Radial Kernel**
```{r}
# build SVM - Radial Kernel classifier 
svm_classifier2 <- ksvm(FGM1 ~ ., data = lebron_train, kernel = "rbfdot", kpar=list())

# make SVM - Radial Kernel predictions 
svm_predictions2 <- predict(svm_classifier2, lebron_test)

# calculate SVM - Radial Kernel accuracy 
round(confusionMatrix(lebron_test$FGM1, svm_predictions2)$overall["Accuracy"]*100, 1)
```

----- 

#####  **Neural Network**
```{r}
# build Neural Network classifer 
ann_classifier <- neuralnet(FGM1 ~ ., data = lebron_train)

# make Neural Network predicitons
ann_test<-lebron_test %>% select(-FGM1)

ann_results <- compute(ann_classifier, ann_test)

ann_predictions<-as.data.frame(ann_results$net.result) %>% 
  mutate(ann_pred=as.factor(ifelse(V2 > V1, 1, 0))) %>% select(ann_pred)

# calculate Neural Network accuracy 
round(confusionMatrix(lebron_test$FGM1, ann_predictions$ann_pred)$overall["Accuracy"]*100, 1)
```


#####  **Neural Network - 5 Hidden Layers**
```{r}
# build Neural Network - 5 Hidden Layers classifer 
ann_classifier2 <- neuralnet(FGM1 ~ ., data = lebron_train, hidden = 5)

# make Neural Network - 5 Hidden Layers predicitons
ann_test<-lebron_test %>% select(-FGM1)

ann_results2 <- compute(ann_classifier2, ann_test)

ann_predictions2<-as.data.frame(ann_results2$net.result) %>% 
  mutate(ann_pred=as.factor(ifelse(V2 > V1, 1, 0))) %>% select(ann_pred)

# calculate Neural Network - 5 Hidden Layers accuracy 
round(confusionMatrix(lebron_test$FGM1, ann_predictions2$ann_pred)$overall["Accuracy"]*100, 1)
```

----- 

#####  **Decision Tree**
```{r fig.width=15}
# build Decision Tree classifier 
dec_classifier <- C5.0(FGM1 ~ ., data = lebron_train)

# make Decision Tree predictions 
dec_predictions <- predict(dec_classifier, lebron_test)

# calculate Decision Tree accuracy 
round(confusionMatrix(lebron_test$FGM1, dec_predictions)$overall["Accuracy"]*100, 1)
```


#####  **Decision Tree - Boosted**
```{r}
# build Decision Tree - Boosted classifier 
dec10_classifier <- C5.0(FGM1 ~ ., data = lebron_train, trials=10)

# make Decision Tree - Boosted predictions 
dec10_predictions <- predict(dec10_classifier, lebron_test)

# calculate Decision Tree - Boosted accuracy 
round(confusionMatrix(lebron_test$FGM1, dec10_predictions)$overall["Accuracy"]*100, 1)
```

----- 

#####  **Model Improvement**

6 of the 7 models yielded an accuracy % greater than the baseline 51%, with the exception of the kNN. Some of the models did exceptionally well, with the **SVM - Vanilla Kernel** and **Decision Tree**, having accuracy %'s of **69%** and **68%**, respectively. In an effort to improve the models, a couple of different techniques should be employed. 

----

#####  **Stacked Model**
A stacked model will be used in an attempt to improve predictions. Each of the the 7 models created previously will be tested on the *training* data set. After this re-testing, these predictions will be used as inputs in a stacked decision tree. 
```{r}
# creating a data from of all predictions made on the testing data
# wil be used later when testing the stacked tree 
testing_predictions<-data.frame(knn_predictions,
                                svm_predictions,
                                svm_predictions2,
                                ann_predictions,
                                ann_predictions2,
                                dec_predictions,
                                dec10_predictions)


# making predictions on training dataset

# ... kNN
knn_predictions_train <- knn(train = knn_train, test = knn_train, cl = knn_train_labels, k=3)

# ... SVM - Vanilla Kernel
svm_predictions_train <- predict(svm_classifier, lebron_train)

# ... SVM - Radial Kernel
svm_predictions_train2 <- predict(svm_classifier2, lebron_train)

# ... Neural Network 
ann_train<-lebron_train %>% select(-FGM1)

ann_results_train <- compute(ann_classifier, ann_train)

ann_predictions_train<-as.data.frame(ann_results_train$net.result) %>%
  mutate(ann_pred=as.factor(ifelse(V2 > V1, 1, 0))) %>% select(ann_pred)

# ... Neural Network - 5 Hidden Layers
ann_results_train2 <- compute(ann_classifier2, ann_train)

ann_predictions_train2<-as.data.frame(ann_results_train2$net.result) %>%
  mutate(ann_pred=as.factor(ifelse(V2 > V1, 1, 0))) %>% select(ann_pred)

# ... Decision Tree
dec_predictions_train <- predict(dec_classifier, lebron_train)

# ... Decision Tree - Boosted
dec10_predictions_train <- predict(dec10_classifier, lebron_train)

# creating a data frame of all predictions made on the training data
training_predictions<-data.frame(knn_predictions_train,
                                 svm_predictions_train,
                                 svm_predictions_train2,
                                 ann_predictions_train,
                                 ann_predictions_train2,
                                 dec_predictions_train,
                                 dec10_predictions_train)
                          

# making names match
names(training_predictions)<-names(testing_predictions)

#  building Stacked Decision Tree
lebron_stacked_tree<-C5.0(lebron_train$FGM1 ~ ., data = training_predictions)

# make Stacked Decision Tree predictions 
stacked_tree_predictions<-predict(lebron_stacked_tree, testing_predictions)


# calculate Stacked Decision Tree accuracy 
round(confusionMatrix(lebron_test$FGM1, stacked_tree_predictions)$overall["Accuracy"]*100, 1)
```

The stacked model yields an accuracy percent lower than some of its components. 

Because the decision tree proved to be among the most accurate classifiers, a tree-based ensembling method should be employed - random forest. 

----

#####  **Random Forest**
```{r}
# build Random Forest classifier 
lebron_rf <- randomForest(FGM1 ~ ., data = lebron_train)

# make Random Forest predictions
rf_predictions<-predict(lebron_rf, lebron_test)

# calculate Random Forest accuracy 
round(confusionMatrix(lebron_test$FGM1, rf_predictions)$overall["Accuracy"]*100, 1)
```

The random forest classifier does a better job than the stacked model, but it is still not as good as the SVM - Vanilla Kernel or the basic decision tree. 

----

#####  **Bagging**
Continuing with tree-based methods, the last esemble method that we will try is a bagged decision tree with 10 fold cross validation. 
```{r}
# set paramters for 10 fold cross validation 
bag_control<-trainControl(method = "cv", number = 10)

# buld Bagged Decision Tree
bagged_dec<-train(FGM1 ~ . , data=lebron_train, method="treebag", trControl=bag_control)

# make Bagged Decision Tree predictions 
bagged_predictions<-predict(bagged_dec, lebron_test)

# calculate Bagged Decision Tree accuracy
round(confusionMatrix(lebron_test$FGM1, bagged_predictions)$overall["Accuracy"]*100, 1)
```


Similarly to the random forest, the bagged decision tree does a better than the stacked model but not as good as the SVM - Vanilla Kernel or the basic decision tree.

----

Despite the improvement attempts, the SVM - Vanilla Kernel remains the most accurate model. To further evaluate the best model and to derive more insights regarding shot selection, we will re-train the SVM on a new player, James Harden. 

----

#####  **Subsetting James Harden**
```{r}
harden<-shot_logs %>% filter(player_name=="james harden")
```

In the games included in the data set, James Harden...

* took **`r nrow(harden)`** total shots 
* took **`r nrow(harden[harden$PTS_TYPE=="2", ] )`** 2 point shots  
* took **`r nrow(harden[harden$PTS_TYPE=="3", ] )`** 3 point shots  
* had a field goal percentage of **`r paste(round(nrow(harden[harden$FGM==1, ]) / nrow(harden) * 100), "%")`**

#####  **Further Data Processing - James Harden**
The same data processing will take place. 
```{r}
# randomizing data & dropping unnecesary columns
harden1<-harden[sample(nrow(harden)), ] %>% 
  select(-MATCHUP, -CLOSEST_DEFENDER, -PTS_TYPE, - player_name, -PERIOD)

# using model.matric to create dummy variables for factors - which will be necesarry for certain ML algorithms
harden1<-as.data.frame(model.matrix(~. -1, data=harden1))

# normalize the data
harden_norm <- as.data.frame(lapply(harden1, normalize))

# re-code variable of interest to factor 
harden_norm$FGM1<-as.factor(harden_norm$FGM1)

# create split point - train on 80% of data, test on 20% of data
split<-round(.80*(nrow(harden_norm)))

# train data
harden_train<-harden_norm[1:split, ]

# test data 
harden_test<-harden_norm[(split+1): nrow(harden_norm), ]

```

With the data processed, we can re-train and re-test the SVM - Vanilla Kernel on Harden. 

----

#####  **SVM - Vanilla Kernel - James Harden**
```{r}
# build SVM - Vanilla Kernel classifier 
svm_classifierHARDEN <- ksvm(FGM1 ~ ., data = harden_train, kernel = "vanilladot", kpar=list())

# make SVM - Vanilla Kernel predcitions
svm_predictionsHARDEN <- predict(svm_classifierHARDEN, harden_test)

# calculate SVM - Vanilla Kernel accuracy 
round(confusionMatrix(harden_test$FGM1, svm_predictionsHARDEN )$overall["Accuracy"]*100, 1)
```

The SVM has an accuracy of 60% for James Harden. If you'll recall, the SVM for Lebron James was 69% accurate. A 9% descrepancy exists in the accuracy of the model. Furthermore, Harden's field goal percentage in the data was 45%, meaning that simply guessing he missed every shot would be 55% accurate. For Harden, The SVM only provides a 5% increase in accuracy. Back to Lebron James, the baseline set for him was 51%, meaning the SVM provided an 18% increase in accuracy. 

From this analysis, a clear question arises. Why are Lebron James's shots much more predictable than James Harden's? One explanation could be shot selection. 

----

#####  **Shot Selection**
To evaluate the shot selection of the players, new variables should be created in order to provide basketball-specific context. 

```{r}
# creating a shot type variable based on shot distance
shot_logs$shot_type <- as.factor(ifelse(shot_logs$SHOT_DIST < 3, "Lay-up/Dunk", 
                                 ifelse(shot_logs$SHOT_DIST < 12, "Short Two", 
                                 ifelse(shot_logs$SHOT_DIST < 22, "Long Two", 
                                 ifelse(shot_logs$SHOT_DIST < 23.9, 
                                        "Longest Two/Three Pointer", "Three Pointer")))))


# creating a variable determining whether it was catch and shoot or dribbling
shot_logs$dribble_or_not <- as.factor(ifelse(shot_logs$DRIBBLES == 0, "Catch & Shoot", "Off Dribble")) 

# creating a variable grouping shot clock positioning
shot_logs$timing <- as.factor(ifelse(shot_logs$SHOT_CLOCK < 3, "Late", 
                                     ifelse(shot_logs$SHOT_CLOCK < 10, "Middle", "Early")))

#Creating a variable defining how open or contested the shot is
shot_logs$openness <- as.factor(ifelse(shot_logs$CLOSE_DEF_DIST < 2, "Heavily Contested", 
                                       ifelse(shot_logs$CLOSE_DEF_DIST < 6, "Contested", "Open")))

# re - creating Lebron and Harden Data 
lebron<-shot_logs %>% filter(player_name=="lebron james")
harden<-shot_logs %>% filter(player_name=="james harden")
```

----

Various data visualizations should portray differences in shot selection, and hopefully provide insight into why Lebron's shots are more predictable. 

#####  **Shooting Overview**
```{r fig.width=15, warning=FALSE, message=FALSE}
lj1<-ggplot(lebron, aes(x = TOUCH_TIME, y = SHOT_DIST, colour = FGM)) + 
     geom_point(alpha = 1, size = 3) + labs(x = "Time With Ball in Hand", y = "Shot Distance") + 
     ggtitle("Lebron - FGM as Product of Distance and Time with Ball") + xlim(0,24) + ylim(0, 35)+
     geom_hline(yintercept = 23.9, linetype = "dashed") +
     geom_text(aes(23,23.9,label = "3 Point Line", vjust = 1), color="black")+ 
     theme_bw()


jh1<-ggplot(harden, aes(x = TOUCH_TIME, y = SHOT_DIST, colour = FGM)) + 
  geom_point(alpha = 1, size = 3) + labs(x = "Time With Ball in Hand", y = "Shot Distance") + 
  ggtitle("Harden - FGM as Product of Distance and Time with Ball") + xlim(0,24) + ylim(0, 35)+
  geom_hline(yintercept = 23.9, linetype = "dashed") + 
  geom_text(aes(23,23.9,label = "3 Point Line", vjust = 1), color="black")+ 
  theme_bw()

grid.arrange(lj1, jh1, ncol=2)
  
```

These graphs provide an overview of each players' shot selection. We see the "made shots" shows in blue, and "missed shots" in red. We see that for close range shots, both Lebron and Harden appear to be highly accurate. These graphs show that both players shoot from all levels, however, they prioritize shots close to the rim and three pointers over mid-range shots. 

*note: 3 point line reflected as constant distance, becase the shooter's horizontal location is not provided in the data*

----

#####  **Shot Selection by Shot Type**
We can more accurately break down their shot selection with bar charts. 
```{r fig.width=15}
lebron_shot_types<-lebron %>% 
  group_by(shot_type) %>% 
  summarize(Percent=round(n()/nrow(lebron)*100, 1))

harden_shot_types<-harden %>% 
  group_by(shot_type) %>% 
  summarize(Percent=round(n()/nrow(harden)*100, 1))

lj_s<-ggplot(lebron_shot_types)+ 
   geom_col(aes(x=shot_type, y=Percent),  fill="navyblue", color="darkred")+ 
   labs(x="Shot Type", title="Lebron Shot % by Type")+
   theme_bw()

jh_s<-ggplot(harden_shot_types)+ 
   geom_col(aes(x=shot_type, y=Percent), fill="red", color="black")+ 
   labs(x="Shot Type", title="Harden Shot % by Type")+
   theme_bw()
  
grid.arrange(lj_s, jh_s, ncol=2)
```

This chart cleary conveys two things: Lebron attempts more short twos, while Harden attempts more three pointers. 

```{r fig.width=15}
lj_fgm<-ggplot(lebron, aes(x = shot_type, fill = FGM)) + 
        geom_bar(position = "fill") + 
        labs(x = "Shot Type", y = "FGM Proportion", title="Lebron - FGM Proportion by Shot Type")+ 
        theme_bw()


jh_fgm<-ggplot(harden, aes(x = shot_type, fill = FGM)) + 
        geom_bar(position = "fill") + 
        labs(x = "Shot Type", y = "FGM Proportion", title="Harden - FGM Proportion by Shot Type")+ 
        theme_bw()

grid.arrange(lj_fgm, jh_fgm, ncol=2)
```

Not only does Lebron attempt more short twos, but he makes a higher percentage of them than Harden does. Lebron also makes a considerably higher percentage of his Layups/Dunk. Given the closeness to the basket for these two shot types, one wouldn't think the discrepancy in FGM is due to talent alone. Let's look to see if there exists differencenes in shot contention. 

----

#####  **Differences in Shot Contention for Layups/Dunks & Short Twos**
```{r fig.width=12}
lebron_contest<-lebron %>% 
  filter(shot_type=="Lay-up/Dunk" | shot_type=="Short Two") %>% 
  group_by(openness) %>% 
  summarize(Percent=round(n()/nrow(lebron)*100, 1))

harden_contest<-harden %>% 
  filter(shot_type=="Lay-up/Dunk" | shot_type=="Short Two") %>% 
  group_by(openness) %>% 
  summarize(Percent=round(n()/nrow(harden)*100, 1))

lj_c<-ggplot(lebron_contest)+ 
   geom_col(aes(x=openness, y=Percent),  fill="navyblue", color="darkred")+ 
   labs(x="Openness", title="Lebron Shot % by Shot Contention - Layups/Dunks & Short Twos")+
   theme_bw()

jh_c<-ggplot(harden_contest)+ 
   geom_col(aes(x=openness, y=Percent), fill="red", color="black")+ 
   labs(x="Openness", title="Harden Shot % by Shot Contention - Layups/Dunks & Short Twos")+
   theme_bw()

grid.arrange(lj_c, jh_c, ncol=2)
```

Interestingly, for layups/dunks and short twos, Harden is heavily contested more than Lebron is, and Harden also experiences less open shots. This likely explains the discrepancy in field goal %. We should now examine the effect shot contention has across all of their respective shots. 

----

#####  **Effect of Shot Contention Across All Shots**
```{r fig.width=15}
lj_c<-ggplot(lebron, aes(x = openness, fill = FGM)) + geom_bar(position = "fill") + 
      labs(x = "Shot Contestion Level", y = "FGM Proportion", 
           title="Lebron - FGM Proportion by Shot Contestion Level") +
  theme_bw()

jh_c<-ggplot(harden, aes(x = openness, fill = FGM)) + geom_bar(position = "fill") + 
      labs(x = "Shot Contestion Level", y = "FGM Proportion", 
           title="Harden - FGM Proportion by Shot Contestion Level") +
  theme_bw()


grid.arrange(lj_c, jh_c, ncol=2)
```

Both players appear to be equally affected by shot contention, with Lebron maybe holding a slight edge in his ability to make contested shots. 

----

Thus far we have uncovered a few potential reasons why Lebron's shots may be more predictable than Harden's: he attempts more layups/dunks and short twos, and also finds himself less heavily contested than Harden on these shots. Lebron likely takes more high-probability shots than Harden, and he converts on them. In trying to explain the discrepancy in model **accuracy**, it is also important to consider the shots Lebron misses. Theoretically, if we are able to more accurately predict the outcome of Lebron's shots, that should mean that Lebron also takes more "bad" shots, and isn't able to convert on them - i.e. He makes the shots he should make, and he misses the shots he should miss

To explore this idea, recall the logistic regression built at the outset of the project.

----

#####  **Assessing Bad Shots**
To assess shot quaility, we can assign probabilities to each shot with logistic regression. The logistic regression seen earlier will be trained on both Lebron and Harden. 
```{r}
log_model_lebron<-glm(FGM ~ DRIBBLES + TOUCH_TIME + SHOT_DIST + CLOSE_DEF_DIST + SHOT_CLOCK +
                      SHOT_NUMBER + FINAL_MARGIN + PERIOD + LOCATION, 
                      data=lebron, family="binomial")


log_model_harden<-glm(FGM ~ DRIBBLES + TOUCH_TIME + SHOT_DIST + CLOSE_DEF_DIST + SHOT_CLOCK +
                      SHOT_NUMBER + FINAL_MARGIN + PERIOD + LOCATION, 
                      data=harden, family="binomial")
```

With the logistic regressions trained, we can use the model to assign probabilities to all shots that Lebron and Harden took. 

```{r}
# making predictions on lebron 
lebron_probs<-predict(log_model_lebron, lebron, type="response")

# storing as data frame 
lebron_probs<-as.data.frame(lebron_probs)


# making predictions on harden
harden_probs<-predict(log_model_harden, harden, type="response")

# storing as data frame 
harden_probs<-as.data.frame(harden_probs)
```

Because Lebron's shots were predicted with much higher accuracy, we should expect that he takes a higher proportion of shots at the "tails" of his distribution. Regarding low probability shots, we will asses shots that had probabilities less than 30%, and then shots that had probabilities of less than 25%. 

```{r}
harden_30<-(sum(harden_probs < .30, na.rm=TRUE) / nrow(harden_probs)) * 100

harden_30

lebron_30<-(sum(lebron_probs < .30, na.rm=TRUE) / nrow(lebron_probs)) * 100

lebron_30

```

```{r}
harden_25<-(sum(harden_probs < .25, na.rm=TRUE) / nrow(harden_probs)) * 100

harden_25

lebron_25<-(sum(lebron_probs < .25, na.rm=TRUE) / nrow(lebron_probs)) * 100

lebron_25

```

While it isn't by a large margin, Lebron actually took a higher proportion of these low probability shots, in both categories, than Harden did. This, combined with what the data visualizations showed, convey a cler message: the SVM was able to more accurately classify Lebron's shot because he takes a higher proportion of both highly probable and highly unprobable shots. 

----

#####  **Summary**

In this project we used various machine learning algorithms to classify shot attempts by Lebron James. After training and testing many models and attempting different model improvement techiques, we settled on an SVM with a Vanilla Kernel as the best classifier. We then re-trained and re-tested this classifier on a different player, James Harden. Becuase a large descrepancy existed in the SVM's ability to classify shots, we employed different data visualizations and a logistic regression to uncover potential reasons why Lebron James's shots are more predictable. 

----
